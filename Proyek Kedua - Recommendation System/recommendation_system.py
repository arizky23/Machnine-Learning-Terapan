# -*- coding: utf-8 -*-
"""Recommendation System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Oanq_nWrUDcqpl_SG4qVtHtpVTIy7dU

# **Data Diri**

---
Nama : Ananda Rizky Nurhidayat

Alamat : Kabupaten Bekasi, Cikarang Selatan

Email : anandakiki1984@gmail.com

SIB Email : M314X0817@dicoding.org

SIB ID : M314X0817

# Introduction

Proyek ini merupakan proyek terakhir dari Dicoding pada kelas Machine Learning Terapan. Pada proyek ini kita akan membuat sebuah **Sistem Rekomendasi** untuk memenuhi submission yang telah diberikan.

# Data Understanding

Kaggle API Setup
"""

!pip install kaggle

"""Upload API token yang berformat .json

> API token ini bisa kita dapatkan dari web kaggle yang sudah kalian loginkan. Ketika anda sudah login, lalu kalian pergi ke bagian akun anda, anda scroll kebawah dan nanti anda akan menemukan Create New API Token pada bagian API
"""

from google.colab import files
files.upload()

"""Memindahkan file kaggle.json kedalam config folder"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# gunakan ls untuk melihat apakah sudah masuk kedalam folder atau belum
!ls ~/.kaggle

"""Mempersiapkan dataset dengan mengunduhnya melalui website kaggle"""

!kaggle datasets download -d arashnic/book-recommendation-dataset

"""Melakukan pengekstrakan terhadap dataset dengan menggunakan perintah unzip"""

!unzip book-recommendation-dataset.zip

"""Terdapat 3 file csv didalamnya, yaitu Books.csv, Ratings.csv, dan Users.csv

Selanjutnya, mari kita baca data-data di atas dengan menggunakan fungsi pandas.read_csv sebagai berikut.
"""

import pandas as pd

books = pd.read_csv('/content/Books.csv')
ratings = pd.read_csv('/content/Ratings.csv')
users = pd.read_csv('/content/Users.csv')

print('Jumlah data penilaian user: ', len(ratings['User-ID'].unique()))
print('Jumlah data user: ', len(users['User-ID'].unique()))

"""Dapat kita lihat bahwa data User-ID yang berada pada dataset Ratings berjumlah 105283 dan jumlah data User dari dataset Users itu sendiri adalah 278858. Ini merupakan data yang sangat banyak dan kita dapat memberikan rekomendasi yang sangat baik pula.

# Univariate Exploratory Data Analysis

Variabel-variabel pada Books Recommendation Dataset adalah sebagai berikut:

- books : merupakan informasi mengenai data buku tersebut.
- ratings : merupakan penilaian terkait buku yang dibaca.
- users : merupakan data pengguna.

Variabel books dan ratings akan digunakan pada model rekomendasi kita. Sedangkan, variabel users hanya untuk melihat data dari si pengguna.

### Books Variabel

Melihat isi dari variabel books
"""

books.head()

"""Kemudian kita explorasi variabel books dengan mengimplementasikan kode berikut."""

books.info()

"""Berdasarkan output di atas, kita dapat mengetahui bahwa file *Books.csv* memiliki **271360 entri**. Dikarenakan memiliki sekali banyak data dalam variabel books ini, maka saya akan memangkas atau mengambil beberapa bagian saja sekitar **4500 entri** dari variabel ini untuk kita gunakan dalam proses sistem rekomendasi ini."""

books = books[1:4501]
books.info()

"""Terlihat disini sudah menjadi **4500 entri**. Selanjutnya, kita akan melihat ada berapa banyak judul buku yang unik serta menampilkan judul apa saja yang terdapat dalam buku tersebut."""

print('Jumlah Judul Buku: ', len(books['Book-Title'].unique()))
print('Judul Buku: ', books['Book-Title'].unique())

"""Terdapat 4355 data buku yang unik dengan judul buku berbeda-beda seperti yang terlihat pada output kode.

### Ratings Variabel

Melihat isi dari variabel ratings
"""

ratings.head()

"""Kemudian kita explorasi variabel ratings dengan mengimplementasikan kode berikut."""

ratings.info()

"""Berdasarkan output di atas, kita dapat mengetahui bahwa file *Ratings.csv* memiliki **1149780 entri**. Kita akan lakukan hal sama seperti variabel books sebelumnya, yaitu dengan mengambil 4500 jumlah entri saja untuk kita gunakan dalam proses sistem rekomendasi ini."""

ratings = ratings[1:4501]
ratings.info()

"""Terlihat disini sudah menjadi **4500 entri**. Selanjutnya kita akan melihat distribusi ratings pada data."""

ratings.describe()

"""Dari output di atas, diketahui bahwa nilai **maksimum rating** adalah **10** dan **nilai minimumnya** adalah **0**. Artinya, skala rating berkisar antara **0** hingga **10**.

Dan kita akan lihat berapa pengguna yang memberikan rating, jumlah ISBN, dan jumlah rating
"""

print('Jumlah ID user: ', len(ratings['User-ID'].unique()))
print('Jumlah ISBN: ', len(ratings['ISBN'].unique()))
print('Jumlah data penilaian buku: ', len(ratings['Book-Rating']))

"""Terdapat 678 jumlah ID user, 4197 jumlah ISBN, dan 4500 jumlah data dari penilaian buku tersebut.

### Users Variabel

kita explorasi variabel users dengan mengimplementasikan kode berikut.
"""

users.info()

"""Berdasarkan output di atas, kita dapat mengetahui bahwa file *Users.csv* memiliki **278858 entri**. Kita akan lakukan hal sama seperti variabel books dan ratings sebelumnya, yaitu dengan mengambil 4500 jumlah entri saja untuk kita gunakan dalam proses sistem rekomendasi ini."""

users = users[1:4501]
users.info()

"""Terlihat disini sudah menjadi **4500 entri**. Selanjutnya kita akan melihat berapa jumlah kolom dan baris pada variabel users."""

print(users.shape)

"""Selanjutnya, mari kita lihat isi dari variabel users."""

users.head()

"""# Data Preprocessing

### Books Variabel

dikarenakan terdapat 3 fitur yang tidak kita gunakan dalam variabel books, maka disini kita akan melakukan drop kolom terhadap ketiganya
"""

books = books.drop(columns=['Image-URL-L', 'Image-URL-M', 'Image-URL-S'],axis=1)
books.head()

"""### Ratings Variabel

> Check Missing Value
"""

# Check missing value dengan fungsi isnull()
ratings.isnull().sum()

"""> Mengetahui jumlah rating"""

# Menghitung jumlah rating, kemudian menggabungkannya berdasarkan ISBN
ratings.groupby('ISBN').sum()

"""### Menggabungkan data dengan fitur penilaian buku

Pertama, definisikan variabel all_books_rate dengan variabel ratings yang telah kita ketahui sebelumnya.
"""

all_books_rate = ratings
all_books_rate

"""Kemudian kita gabungkan data books yang berisikan ISBN, Book-Title, dan Book-Author berdasarkan ISBN dan memasukkannya ke dalam variabel all_books_name"""

# Menggabungkan all_books_rate dengan dataframe books berdasarkan ISBN, Book-Title, dan Book-Author
all_books = pd.merge(all_books_rate, books[['ISBN','Book-Title','Book-Author']], on='ISBN', how='left')
all_books

"""### Users Variabel

Disini kita akan menggabungkan seluruh data User-ID yang terdapat pada variabel users dan ratings
"""

import numpy as np

# Menggabungkan seluruh User-ID
user_all = np.concatenate((
    users['User-ID'].unique(),
    ratings['User-ID'].unique()
))

# Mengurutkan data yang sama kemudian mengurutkannya
user_all = np.sort(np.unique(user_all))

print('Jumlah seluruh data user: ', len(user_all))

"""# Data Preparation

### Mengatasi Missing Value

Setelah proses penggabungan, mari kita cek lagi datanya apakah ada missing value atau tidak.
"""

all_books.isnull().sum()

"""Dapat kita lihat disini, ternyata terdapat 3837 missing value pada fitur Book-Title dan Book-Author. Oleh karena itu kita akan drop missing value yang terdapat pada Book-Title dan Book-Author tersebut."""

all_books_clean = all_books.dropna()
all_books_clean

"""Perhatikanlah, data kita sekarang memiliki 663 baris. Lalu kita cek kembali untuk memastikan bahwa sudah tidak memiliki missing value lagi dalam data."""

# Mengecek kembali missing value pada variabel all_books_clean
all_books_clean.isnull().sum()

len(all_books)

"""Selanjutnya kita akan masuk ke tahap persiapan. Kita akan mengurutkan buku berdasarkan nomor ISBN dan memasukkannya ke dalam variabel preparation."""

# Mengurutkan buku berdasarkan ISBN kemudian memasukkannya ke dalam variabel preparation
preparation = all_books_clean.sort_values('ISBN', ascending=True)
preparation

"""Selanjutnya, kita akan membuang data duplikat pada kolom ISBN."""

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('ISBN')
preparation

"""Selanjutnya, kita perlu melakukan konversi data series menjadi list. Dalam hal ini, kita menggunakan fungsi tolist() dari library **numpy**."""

# Mengonversi data series 'ISBN' menjadi dalam bentuk list
book_id = preparation['ISBN'].tolist()

# Mengonversi data series 'Book-Title' menjadi dalam bentuk list
book_title = preparation['Book-Title'].tolist()

# Mengonversi data series 'Book-Author' menjadi dalam bentuk list
book_author = preparation['Book-Author'].tolist()

print(len(book_id))
print(len(book_title))
print(len(book_author))

"""Tahap berikutnya, kita akan membuat dictionary untuk menentukan pasangan key-value pada data book_id, book_title, dan book_author yang telah kita siapkan sebelumnya."""

book_new = pd.DataFrame({
    'book_id': book_id,
    'book_title': book_title,
    'book_author': book_author
})
book_new

"""# Model Development dengan Content Based Filtering

### TF-IDF Vectorizer

Pada tahap ini, kita akan membangun sistem rekomendasi menggunakan teknik TF-IDF Vectorizer yang dimana teknik tersebut digunakan untuk menemukan representasi fitur penting dari setiap kategori buku. untuk menggunakannya kita gunakan fungsi tfidfvectorizer() dari library sklearn.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()
 
# Melakukan perhitungan idf pada data 'book_title'
tf.fit(book_new['book_title']) 
 
# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names()

"""Selanjutnya, lakukan fit dan transformasi ke dalam bentuk matriks. """

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(book_new['book_title']) 
 
# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Perhatikanlah, matriks yang kita miliki berukuran **(493, 1081)**. Nilai **493** merupakan **ukuran data** dan **1081** merupakan matrik **kategori buku**. 

Untuk menghasilkan **vektor tf-idf** dalam bentuk matriks, kita menggunakan fungsi todense() sebagai berikut.
"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Selanjutnya, mari kita lihat matriks **tf-idf** untuk beberapa **kategori buku**."""

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan judul buku
# Baris diisi dengan author buku
 
pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=book_new.book_author
).sample(1081, axis=1).sample(10, axis=0)

"""### Cosine Similarity

Sekarang, kita akan menghitung derajat kesamaan (similarity degree) antar buku dengan teknik cosine similarity. Di sini, kita menggunakan fungsi cosine_similarity dari library sklearn.
"""

from sklearn.metrics.pairwise import cosine_similarity

# menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Selanjutnya, mari kita lihat matriks kesamaan setiap book author dengan menampilkan book author dalam 7 sampel kolom (axis = 1) dan 10 sampel baris (axis=0). Jalankan kode berikut."""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa author buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=book_new['book_author'], columns=book_new['book_author'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap author buku
cosine_sim_df.sample(7, axis=1).sample(10, axis=0)

"""### Mendapatkan Rekomendasi"""

def book_recommendations(nama_author, similarity_data=cosine_sim_df, items=book_new[['book_title', 'book_author']], k=10):
    
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan    
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_author].to_numpy().argpartition(
        range(-1, -k, -1))
    
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # Drop nama_author agar nama buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_author, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

"""Selanjutnya, mari kita terapkan kode berikut untuk menemukan rekomendasi buku yang mirip dengan author dari Andre Dubus III."""

book_new[book_new.book_author.eq('Andre Dubus III')]

"""Nah, sekarang, dapatkan book recommendation dengan memanggil fungsi yang telah kita definisikan sebelumnya:"""

# Mendapatkan rekomendasi buku yang mirip dengan buku dari author Andre Dubus III
book_recommendations('Andre Dubus III')

"""Yay! Berhasil! Sistem kita memberikan rekomendasi 10 nama buku dengan kategori book_author Mark Z. Danielewski, William Sleator dan James Patterson

# Model Development dengan Collaborative Filtering

### Data Understanding

Pada proyek ini, kita akan melakukan impor library di awal agar terlihat rapi dan pada sel kode selanjutnya, kita bisa fokus pada menulis kodenya saja.
"""

# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""Selanjutnya, pahami terlebih dahulu data rating yang kita miliki. Saat itu, Anda membuat variabel ratings dan menetapkan data pada variabel tersebut. Untuk memudahkan supaya tidak tertukar dengan fitur **‘rating** pada data, kita ubah nama variabel rating menjadi df."""

# membaca dataset
df = ratings
df

"""Perhatikanlah, data rating memiliki 4500 baris dan 3 kolom.

### Data Preparation

Kini Anda memasuki tahap preprocessing. Pada tahap ini, kita akan melakukan persiapan data untuk menyandikan (encode) fitur ‘User-ID’ dan ‘ISBN’ ke dalam indeks integer.
"""

# Mengubah User-ID menjadi list tanpa nilai yang sama
user_ids = df['User-ID'].unique().tolist()
print('list ID User: ', user_ids)
 
# Melakukan encoding User-ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded ID User: ', user_to_user_encoded)
 
# Melakukan proses encoding angka ke ke User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke ID User: ', user_encoded_to_user)

"""Selanjutnya, lakukan hal yang sama pada fitur ‘ISBN’."""

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_ids = df['ISBN'].unique().tolist()
 
# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
 
# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

"""Selanjutnya, petakan User-ID dan ISBN ke dataframe yang berkaitan."""

# Mapping User-ID ke dataframe user
df['user'] = df['User-ID'].map(user_to_user_encoded)
 
# Mapping ISBN ke dataframe buku
df['book'] = df['ISBN'].map(book_to_book_encoded)

"""Terakhir, cek beberapa hal dalam data seperti jumlah user, jumlah buku, dan mengubah nilai rating menjadi float."""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah buku
num_book = len(book_encoded_to_book)
print(num_book)
 
# Mengubah rating menjadi nilai float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df['Book-Rating'])
 
# Nilai maksimal rating
max_rating = max(df['Book-Rating'])
 
print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""### Membagi Data untuk Training dan Validasi

Anda tentu sudah menduga, pada tahap ini kita akan melakukan pembagian data menjadi data training dan validasi. Betul! Namun sebelumnya, acak datanya terlebih dahulu agar distribusinya menjadi random.
"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""Selanjutnya, kita bagi data **train** dan **validasi** dengan komposisi 80:20. Namun sebelumnya, kita perlu **memetakan (mapping)** data **user** dan **buku** menjadi satu value terlebih dahulu. Lalu, buatlah rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training."""

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""### Proses Training

Pada tahap ini, model menghitung skor kecocokan antara pengguna dan buku dengan teknik embedding. Pertama, kita melakukan proses embedding terhadap data user dan book. Selanjutnya, lakukan operasi perkalian dot product antara embedding user dan resto. Selain itu, kita juga dapat menambahkan bias untuk setiap user dan resto. Skor kecocokan ditetapkan dalam skala [0,1] dengan fungsi aktivasi sigmoid.

Di sini, kita membuat class RecommenderNet dengan keras Model class. Kode class RecommenderNet ini terinspirasi dari tutorial dalam situs Keras dengan beberapa adaptasi sesuai kasus yang sedang kita selesaikan. Terapkan kode berikut.
"""

class RecommenderNet(tf.keras.Model):
  
  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_book = tf.tensordot(user_vector, book_vector, 2) 
 
    x = dot_user_book + user_bias + book_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

"""Selanjutnya, lakukan proses compile terhadap model."""

model = RecommenderNet(num_users, num_book, 50) # Inisialisasi model

# Model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Model ini menggunakan Binary Crossentropy untuk menghitung **loss function**, Adam (Adaptive Moment Estimation) sebagai **optimizer**, dan root mean squared error (RMSE) sebagai **metrics evaluation**. 

Langkah berikutnya, mulailah proses training.
"""

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""### Mendapatkan Rekomendasi Buku

Sebelumnya, pengguna telah memberi rating pada beberapa resto yang telah mereka berikan. Kita menggunakan rating ini untuk membuat rekomendasi buku yang mungkin cocok untuk pengguna. Nah, buku yang akan direkomendasikan tentulah buku yang belum pernah diberikan oleh pengguna. Oleh karena itu, kita perlu membuat variabel book_not_visited sebagai daftar buku untuk direkomendasikan pada pengguna.

Variabel **book_not_visited** diperoleh dengan menggunakan operator bitwise (~) pada variabel **book_visited_by_user**.

Terapkan kode berikut.
"""

book_df = book_new
data = pd.read_csv('/content/Ratings.csv')
df = data[1:4501]

# Mengambil sample user
user_id = df['User-ID'].sample(50).iloc[0]
book_visited_by_user = df[df['User-ID'] == user_id]
 
# Operator bitwise (~)
book_not_visited = book_df[~book_df['book_id'].isin(book_visited_by_user.ISBN.values)]['book_id'] 
book_not_visited = list(
    set(book_not_visited)
    .intersection(set(book_to_book_encoded.keys()))
)
 
book_not_visited = [[book_to_book_encoded.get(x)] for x in book_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_visited), book_not_visited)
)

"""Selanjutnya, untuk memperoleh rekomendasi buku, gunakan fungsi model.predict() dari library **Keras** dengan menerapkan kode berikut."""

rating = model.predict(user_book_array).flatten()
 
top_rating_indices = rating.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_visited[x][0]) for x in top_rating_indices
]
 
print('Showing book by author recommendations for users: {}'.format(user_id))
top_book_user = (
    book_visited_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
book_df_rows = book_df[book_df['book_id'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.book_author, ':', row.book_title)
 
print('---' * 12)
print('Top 10 Book Author Recommendation')
print('---' * 12)
 
recommended_book = book_df[book_df['book_id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.book_author, ':', row.book_title)

"""Selamat! Anda telah berhasil memberikan rekomendasi kepada user. Sebagai contoh, hasil di atas adalah rekomendasi untuk user dengan id 277962. Dari output tersebut, kita telah memberikan rekomendasi buku berdasarkan pengarangnya.

## Evaluasi

Untuk melihat visualisasi proses training, mari kita plot metrik evaluasi dengan matplotlib.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Perhatikanlah, proses training model cukup smooth dan model konvergen pada epochs sekitar 100. Dari proses ini, kita memperoleh nilai error akhir pada data latih sebesar sekitar 0.13 dan error pada data validasi sebesar 0.38. Nilai tersebut cukup bagus untuk sistem rekomendasi."""